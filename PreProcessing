# FOR NULL VALUES
# Get names of columns with missing values
missingColls = [col for col in X_train.columns if X_train[col].isna().any()]

# Drop columns in training and validation data
reduced_X_train = X_train.drop(missingColls, axis = 1)
reduced_X_valid = X_valid.drop(missingColls, axis = 1)

#Imputation
from sklearn.impute import SimpleImputer

Imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(Imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(Imputer.transform(X_valid))

# Imputation removed column names; put them back
imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns

# FOR CATEGORICAL DATA

# Get number of unique entries in each column with categorical data
object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))
d = dict(zip(object_cols, object_nunique))

# Print number of unique entries by column, in ascending order
sorted(d.items(), key=lambda x: x[1])




# Drop columns in training and validation data
drop_X_train = X_train.select_dtypes(exclude = ["object"])
drop_X_valid = X_valid.select_dtypes(exclude = ["object"])




# Ordinal Encoding
# Categorical columns in the training data
object_cols = [col for col in X_train.columns if X_train[col].dtype == "object"]

# Columns that can be safely ordinal encoded
good_label_cols = [col for col in object_cols if 
                   set(X_valid[col]).issubset(set(X_train[col]))]
        

# Problematic columns that will be dropped from the dataset
bad_label_cols = list(set(object_cols)-set(good_label_cols))
print('Categorical columns that will be ordinal encoded:', good_label_cols)
print('\nCategorical columns that will be dropped from the dataset:', bad_label_cols)

from sklearn.preprocessing import OrdinalEncoder

# Drop categorical columns that will not be encoded
label_X_train = X_train.drop(bad_label_cols, axis=1)
label_X_valid = X_valid.drop(bad_label_cols, axis=1)

# Apply ordinal encoder 
OEncoder = OrdinalEncoder()
label_X_train[good_label_cols] = OEncoder.fit_transform(label_X_train[good_label_cols])
label_X_valid[good_label_cols] = OEncoder.transform(label_X_valid[good_label_cols])




# ONE-HOT ENCODING

# Columns that will be one-hot encoded
low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]

# Columns that will be dropped from the dataset
high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))

print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)
print('\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)



from sklearn.preprocessing import OneHotEncoder

# Drop the columns with too high cardinality
drp_X_train = X_train.drop(high_cardinality_cols, axis = 1)
drp_X_valid = X_valid.drop(high_cardinality_cols, axis = 1)

# Train OHEncoder on the data
OHEncoder = OneHotEncoder(handle_unknown = "ignore", sparse = False)
OH_cols_train = pd.DataFrame(OHEncoder.fit_transform(drp_X_train[low_cardinality_cols]))
OH_cols_valid = pd.DataFrame(OHEncoder.transform(drp_X_valid[low_cardinality_cols]))

# One-hot encoding removed index; put it back
OH_cols_train.index = drp_X_train.index
OH_cols_valid.index = drp_X_valid.index

# Remove categorical columns (will replace with one-hot encoding)
num_X_train = drp_X_train.drop(low_cardinality_cols, axis=1)
num_X_valid = drp_X_valid.drop(low_cardinality_cols, axis=1)

OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1) # Your code here
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1) # Your code here

# Make sure that every feature is in string form
OH_X_train.columns = OH_X_train.columns.astype(str)
OH_X_valid.columns = OH_X_valid.columns.astype(str)



# One-hot encode Categorical feature, adding a column prefix "Cat"
X_new = pd.get_dummies(df.Categorical, prefix="Cat")




#EXAMPLES OF CLEANING UP DATA

customer["AverageIncome"] = (
    customer.groupby("State")  # for each state
    ["Income"]                 # select the income
    .transform("mean")         # and compute its mean
)

customer[["State", "Income", "AverageIncome"]].head(10) 


customer[["Type", "Level"]] = (  # Create two new features
    customer["Policy"]           # from the Policy feature
    .str                         # through the string accessor
    .split(" ", expand=True)     # by splitting on " "
                                 # and expanding the result into separate columns
)

customer[["Policy", "Type", "Level"]].head(10)


components = [ "Cement", "BlastFurnaceSlag", "FlyAsh", "Water",
               "Superplasticizer", "CoarseAggregate", "FineAggregate"]
concrete["Components"] = concrete[components].gt(0).sum(axis=1)

concrete[components + ["Components"]].head(10)
#Sum up components


# Use as many ratios, as they are hard for models to learn









#CLUSTERING

from sklearn.cluster import KMeans

X_po = X[features_to_be_used_in_clustering]

kmeans = KMeans(n_clusters=6)
X["Cluster"] = kmeans.fit_predict(X_po)
X["Cluster"] = X["Cluster"].astype("category")


#Dates
# create a new column, date_parsed, with the parsed dates
landslides['date_parsed'] = pd.to_datetime(landslides['date'], format="%m/%d/%y")